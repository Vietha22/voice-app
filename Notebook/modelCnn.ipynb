{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Deep Learning Jump Start\n# \n# Music Genre Classifier ","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:43.236062Z","iopub.execute_input":"2022-06-14T03:34:43.236641Z","iopub.status.idle":"2022-06-14T03:34:43.239694Z","shell.execute_reply.started":"2022-06-14T03:34:43.236605Z","shell.execute_reply":"2022-06-14T03:34:43.238996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os, json, math, librosa\n\nimport IPython.display as ipd\nimport librosa.display\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D\n\nimport sklearn.model_selection as sk\n\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:43.251356Z","iopub.execute_input":"2022-06-14T03:34:43.251913Z","iopub.status.idle":"2022-06-14T03:34:43.25982Z","shell.execute_reply.started":"2022-06-14T03:34:43.251867Z","shell.execute_reply":"2022-06-14T03:34:43.2589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:43.267459Z","iopub.execute_input":"2022-06-14T03:34:43.268056Z","iopub.status.idle":"2022-06-14T03:34:44.065367Z","shell.execute_reply.started":"2022-06-14T03:34:43.268005Z","shell.execute_reply":"2022-06-14T03:34:44.063887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting Genres from folder name\n\nMUSIC = '../input/gtzan-dataset-music-genre-classification/Data/genres_original/'\nmusic_dataset = [] # File locations for each wav file \ngenre_target = [] # \nfor root, dirs, files in os.walk(MUSIC):\n    for name in files:\n        filename = os.path.join(root, name)\n        if filename != '/data/genres_original/jazz/jazz.00054.wav':\n            music_dataset.append(filename)\n            genre_target.append(filename.split(\"/\")[5])\n            ","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:44.067745Z","iopub.execute_input":"2022-06-14T03:34:44.068177Z","iopub.status.idle":"2022-06-14T03:34:44.230803Z","shell.execute_reply.started":"2022-06-14T03:34:44.068136Z","shell.execute_reply":"2022-06-14T03:34:44.229739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying genres \n\nprint(set(genre_target))","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:44.232597Z","iopub.execute_input":"2022-06-14T03:34:44.232895Z","iopub.status.idle":"2022-06-14T03:34:44.238973Z","shell.execute_reply.started":"2022-06-14T03:34:44.232854Z","shell.execute_reply":"2022-06-14T03:34:44.237974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing Audio Files\n\naudio_path = music_dataset[500]\n# img_path = './data/images_original/blues/blues00011.png'\n\nx , sr = librosa.load(audio_path)\n\n\n\nlibrosa.load(audio_path, sr=None)\n\nipd.Audio(audio_path)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:44.242444Z","iopub.execute_input":"2022-06-14T03:34:44.243155Z","iopub.status.idle":"2022-06-14T03:34:44.33894Z","shell.execute_reply.started":"2022-06-14T03:34:44.24311Z","shell.execute_reply":"2022-06-14T03:34:44.338196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing Audio File as a waveform\nplt.figure(figsize=(16, 5))\nlibrosa.display.waveplot(x, sr=sr)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:44.340005Z","iopub.execute_input":"2022-06-14T03:34:44.340294Z","iopub.status.idle":"2022-06-14T03:34:44.903469Z","shell.execute_reply.started":"2022-06-14T03:34:44.340246Z","shell.execute_reply":"2022-06-14T03:34:44.902243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing audio file as a spectogram\nX = librosa.stft(x)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.title('Spectogram')\nplt.colorbar()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:44.905137Z","iopub.execute_input":"2022-06-14T03:34:44.905493Z","iopub.status.idle":"2022-06-14T03:34:46.581065Z","shell.execute_reply.started":"2022-06-14T03:34:44.905448Z","shell.execute_reply":"2022-06-14T03:34:46.580307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing Audio as Mel-Spectogram\n\nfile_location = audio_path\ny, sr = librosa.load(file_location)\nmelSpec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\nmelSpec_dB = librosa.power_to_db(melSpec, ref=np.max)\nplt.figure(figsize=(10, 5))\nlibrosa.display.specshow(melSpec_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\nplt.colorbar(format='%+1.0f dB')\nplt.title(\"MelSpectrogram\")\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:46.582287Z","iopub.execute_input":"2022-06-14T03:34:46.582955Z","iopub.status.idle":"2022-06-14T03:34:47.220658Z","shell.execute_reply.started":"2022-06-14T03:34:46.582917Z","shell.execute_reply":"2022-06-14T03:34:47.219647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET_PATH = '../input/gtzan-dataset-music-genre-classification/Data/genres_original/'\nJSON_PATH = \"data_10.json\"\nSAMPLE_RATE = 22050\nTRACK_DURATION = 30 # measured in seconds\nSAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n\n\ndef save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n    \"\"\"Extracts MFCCs from music dataset and saves them into a json file along witgh genre labels.\n        :param dataset_path (str): Path to dataset\n        :param json_path (str): Path to json file used to save MFCCs\n        :param num_mfcc (int): Number of coefficients to extract\n        :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples\n        :param hop_length (int): Sliding window for FFT. Measured in # of samples\n        :param: num_segments (int): Number of segments we want to divide sample tracks into\n        :return:\n        \"\"\"\n\n    # dictionary to store mapping, labels, and MFCCs\n    data = {\n        \"mapping\": [],\n        \"labels\": [],\n        \"mfcc\": []\n    }\n\n    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n\n    # loop through all genre sub-folder\n    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n\n        # ensure we're processing a genre sub-folder level\n        if dirpath is not dataset_path:\n\n            # save genre label (i.e., sub-folder name) in the mapping\n            semantic_label = dirpath.split(\"/\")[-1]\n            data[\"mapping\"].append(semantic_label)\n            print(\"\\nProcessing: {}\".format(semantic_label))\n\n            # process all audio files in genre sub-dir\n            for f in filenames:\n\n\t\t# load audio file\n\n                file_path = os.path.join(dirpath, f)\n            \n                if file_path != '../input/gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav':\n\n                    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n                \n                \n                    # process all segments of audio file\n                    for d in range(num_segments):\n\n                        # calculate start and finish sample for current segment\n                        start = samples_per_segment * d\n                        finish = start + samples_per_segment\n\n                        # extract mfcc\n                        mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n                        mfcc = mfcc.T\n\n                        # store only mfcc feature with expected number of vectors\n                        if len(mfcc) == num_mfcc_vectors_per_segment:\n                            data[\"mfcc\"].append(mfcc.tolist())\n                            data[\"labels\"].append(i-1)\n                            print(\"{}, segment:{}\".format(file_path, d+1))\n\n    # save MFCCs to json file\n    with open(json_path, \"w\") as fp:\n        json.dump(data, fp, indent=4)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:47.222196Z","iopub.execute_input":"2022-06-14T03:34:47.222811Z","iopub.status.idle":"2022-06-14T03:34:47.2377Z","shell.execute_reply.started":"2022-06-14T03:34:47.222772Z","shell.execute_reply":"2022-06-14T03:34:47.236781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Runs Data Processing     \nsave_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:34:47.239005Z","iopub.execute_input":"2022-06-14T03:34:47.239255Z","iopub.status.idle":"2022-06-14T03:38:53.832708Z","shell.execute_reply.started":"2022-06-14T03:34:47.239225Z","shell.execute_reply":"2022-06-14T03:38:53.831031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nDATA_PATH = \"./data_10.json\"\n\n\ndef load_data(data_path):\n    \"\"\"Loads training dataset from json file.\n        :param data_path (str): Path to json file containing data\n        :return X (ndarray): Inputs\n        :return y (ndarray): Targets\n    \"\"\"\n\n    with open(data_path, \"r\") as fp:\n        data = json.load(fp)\n\n    X = np.array(data[\"mfcc\"])\n    y = np.array(data[\"labels\"])\n    z = np.array(data['mapping'])\n    return X, y, z\n\n\ndef plot_history(history):\n    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n        :param history: Training history of model\n        :return:\n    \"\"\"\n\n    fig, axs = plt.subplots(2)\n\n    # create accuracy sublpot\n    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n    axs[0].set_ylabel(\"Accuracy\")\n    axs[0].legend(loc=\"lower right\")\n    axs[0].set_title(\"Accuracy eval\")\n\n    # create error sublpot\n    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n    axs[1].set_ylabel(\"Error\")\n    axs[1].set_xlabel(\"Epoch\")\n    axs[1].legend(loc=\"upper right\")\n    axs[1].set_title(\"Error eval\")\n\n    plt.show()\n\n\ndef prepare_datasets(test_size, validation_size):\n    \"\"\"Loads data and splits it into train, validation and test sets.\n    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split\n    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split\n    :return X_train (ndarray): Input training set\n    :return X_validation (ndarray): Input validation set\n    :return X_test (ndarray): Input test set\n    :return y_train (ndarray): Target training set\n    :return y_validation (ndarray): Target validation set\n    :return y_test (ndarray): Target test set\n    :return z : Mappings for data\n    \"\"\"\n\n    # load data\n    X, y, z = load_data(DATA_PATH)\n\n    # create train, validation and test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n\n    # add an axis to input sets\n    X_train = X_train[..., np.newaxis]\n    X_validation = X_validation[..., np.newaxis]\n    X_test = X_test[..., np.newaxis]\n\n    return X_train, X_validation, X_test, y_train, y_validation, y_test, z\n\n\ndef build_model(input_shape):\n    \"\"\"Generates CNN model\n    :param input_shape (tuple): Shape of input set\n    :return model: CNN model\n    \"\"\"\n\n    # build network topology\n    model = keras.Sequential()\n\n    # 1st conv layer\n    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n    model.add(keras.layers.BatchNormalization())\n\n    # 2nd conv layer\n    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n    model.add(keras.layers.BatchNormalization())\n\n    # 3rd conv layer\n    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu'))\n    model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n    model.add(keras.layers.BatchNormalization())\n\n    # flatten output and feed it into dense layer\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(64, activation='relu'))\n    model.add(keras.layers.Dropout(0.3))\n\n    # output layer\n    model.add(keras.layers.Dense(10, activation='softmax'))\n\n    return model\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:38:53.838988Z","iopub.execute_input":"2022-06-14T03:38:53.839461Z","iopub.status.idle":"2022-06-14T03:38:53.865752Z","shell.execute_reply.started":"2022-06-14T03:38:53.8394Z","shell.execute_reply":"2022-06-14T03:38:53.864603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get train, validation, test splits\nX_train, X_validation, X_test, y_train, y_validation, y_test, z = prepare_datasets(0.25, 0.2)\n\n# create network\ninput_shape = (X_train.shape[1], X_train.shape[2], 1)\nmodel = build_model(input_shape)\n\n# compile model\noptimiser = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimiser,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n# train model\nhistory = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=128, epochs=700)\n\n# plot accuracy/error for training and validation\nplot_history(history)\n\n# evaluate model on test set\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\nprint('\\nTest accuracy:', test_acc)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T03:38:53.86754Z","iopub.execute_input":"2022-06-14T03:38:53.868261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef predict(model, file_path):\n    \"\"\"Predict a single sample using the trained model\n    :param model: Trained classifier\n    :param X: Input data\n    :param y (int): Target\n    \"\"\"\n    data = []\n    label = ['disco','metal','reggae', 'blues', 'rock', 'classical', 'jazz', 'hiphop', 'country', 'pop']\n    num_segments = 10\n    hop_length = 512\n    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n    \n    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n    for d in range(num_segments):\n\n        # calculate start and finish sample for current segment\n        start = samples_per_segment * d\n        finish = start + samples_per_segment\n\n        # extract mfcc\n        mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=13, n_fft=2048, hop_length=hop_length)\n        mfcc = mfcc.T\n\n        # store only mfcc feature with expected number of vectors\n        if len(mfcc) == num_mfcc_vectors_per_segment:\n            data.append(mfcc.tolist())\n    \n    # add a dimension to input data for sample - model.predict() expects a 4d array in this case\n    X = np.array(data)\n    X = X[...,np.newaxis]\n#     X = X[np.newaxis, ...] # array shape (1, 130, 13, 1)\n    print(X.shape)\n\n    # perform prediction\n    prediction = model.predict(X)\n\n    # get index with max value\n    predicted_index = np.argmax(prediction, axis=1)\n    predicted_index = predicted_index.reshape(10)\n    # get mappings for target and predicted label\n    elements_count = {0:0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n    for element in predicted_index:\n        if element in elements_count:\n            # incerementing the count by 1\n            elements_count[element] += 1\n    print(elements_count)\n\n    print(\"Predicted label:\",predicted_index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pick a sample to predict from the test set\n\npredict(model,'../input/teeeee/3_class.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}